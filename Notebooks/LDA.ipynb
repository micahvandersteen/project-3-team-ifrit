{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline topic analysis with LDA\n",
    "The purpose of this analysis is to find dominant topics across news headlines (25 per day, over 1 year). These topics will later be correlated with daily stock market loss/gain to understand if certain topics influence the stock market.\n",
    "\n",
    "This work borrows heavily from Susan Li's [\"Topic Modeling and LDA in Python\"](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/stacy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"../Data/RedditNews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "\n",
    "# 73,608 records from 2016-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News\n",
       "0  A 117-year-old woman in Mexico City finally re...\n",
       "1   IMF chief backs Athens as permanent Olympic host\n",
       "2  The president of France says if Brexit won, so...\n",
       "3  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting just the headlines for our corpus\n",
    "headlines = data[['News']]\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "### Lemmitize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the words keeping the context (stemming is \"dumb\" so we won't)\n",
    "# However if we have a much larger corpus, we might consider stemming (as it is faster)\n",
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v') # pos='v' means it peforms stemming with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords and words shorter than 3 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and words shorter than 3 characters, then lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['The', 'president', 'of', 'France', 'says', 'if', 'Brexit', 'won,', 'so', 'can', 'Donald', 'Trump']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['president', 'france', 'say', 'brexit', 'donald', 'trump']\n"
     ]
    }
   ],
   "source": [
    "sample = headlines['News'][2]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the headlines and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [year, woman, mexico, city, finally, receive, ...\n",
       "1      [chief, back, athens, permanent, olympic, host]\n",
       "2      [president, france, say, brexit, donald, trump]\n",
       "3    [british, police, hours, notice, threaten, hun...\n",
       "4    [nobel, laureates, urge, greenpeace, stop, opp...\n",
       "Name: News, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_headlines = headlines['News'].map(preprocess)\n",
    "cleaned_headlines[:5] # Check the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the word occurences using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alvarez\n",
      "1 bear\n",
      "2 birth\n",
      "3 certificate\n",
      "4 city\n",
      "5 die\n",
      "6 finally\n",
      "7 hours\n",
      "8 later\n",
      "9 lira\n",
      "10 mexico\n"
     ]
    }
   ],
   "source": [
    "# corpora.Dictionary implements the concept of a Dictionary – a mapping between words and their integer ids.\n",
    "# https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "dictionary = gensim.corpora.Dictionary(cleaned_headlines)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out irrelevant words\n",
    "'''\n",
    "    less than 15 documents (absolute number) or\n",
    "    more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "    after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "'''\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier.\n",
    "'''\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in cleaned_headlines]\n",
    "bow_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 21 (\"brexit\") appears 1 time.\n",
      "Word 22 (\"donald\") appears 1 time.\n",
      "Word 23 (\"france\") appears 1 time.\n",
      "Word 24 (\"president\") appears 1 time.\n",
      "Word 25 (\"say\") appears 1 time.\n",
      "Word 26 (\"trump\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_2 = bow_corpus[2]\n",
    "\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                            dictionary[bow_doc_2[i][0]], \n",
    "                                            bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2600564898594514),\n",
      " (1, 0.28994526204184407),\n",
      " (2, 0.3744826130954538),\n",
      " (3, 0.2051174112907887),\n",
      " (4, 0.21074667376840217),\n",
      " (5, 0.2818126498910203),\n",
      " (6, 0.24752380792284462),\n",
      " (7, 0.2842753904922502),\n",
      " (8, 0.2101120460560261),\n",
      " (9, 0.32010680937189695),\n",
      " (10, 0.253807102500421),\n",
      " (11, 0.3026294517497535),\n",
      " (12, 0.20803861949214233),\n",
      " (13, 0.16333461952636777),\n",
      " (14, 0.16583903979070258)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.045*\"israel\" + 0.020*\"israeli\" + 0.018*\"china\" + 0.018*\"state\" + 0.015*\"right\" + 0.014*\"say\" + 0.014*\"human\" + 0.011*\"palestinian\" + 0.010*\"unite\" + 0.010*\"iran\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"bank\" + 0.013*\"government\" + 0.010*\"protest\" + 0.009*\"court\" + 0.008*\"chinese\" + 0.008*\"china\" + 0.008*\"million\" + 0.008*\"police\" + 0.006*\"people\" + 0.006*\"billion\"\n",
      "Topic: 2 \n",
      "Words: 0.034*\"kill\" + 0.021*\"attack\" + 0.016*\"bomb\" + 0.014*\"force\" + 0.010*\"army\" + 0.010*\"soldier\" + 0.010*\"troop\" + 0.009*\"pakistan\" + 0.008*\"taliban\" + 0.008*\"police\"\n",
      "Topic: 3 \n",
      "Words: 0.023*\"north\" + 0.022*\"korea\" + 0.021*\"kill\" + 0.019*\"south\" + 0.015*\"pakistan\" + 0.011*\"strike\" + 0.008*\"year\" + 0.008*\"death\" + 0.008*\"children\" + 0.007*\"say\"\n",
      "Topic: 4 \n",
      "Words: 0.036*\"gaza\" + 0.020*\"israel\" + 0.013*\"ship\" + 0.013*\"hamas\" + 0.009*\"israeli\" + 0.007*\"children\" + 0.006*\"say\" + 0.006*\"georgia\" + 0.006*\"sell\" + 0.006*\"report\"\n",
      "Topic: 5 \n",
      "Words: 0.017*\"world\" + 0.010*\"india\" + 0.009*\"power\" + 0.009*\"water\" + 0.008*\"japan\" + 0.007*\"plant\" + 0.007*\"food\" + 0.007*\"global\" + 0.007*\"years\" + 0.006*\"saudi\"\n",
      "Topic: 6 \n",
      "Words: 0.026*\"police\" + 0.013*\"president\" + 0.013*\"russian\" + 0.009*\"russia\" + 0.009*\"protest\" + 0.008*\"woman\" + 0.008*\"protesters\" + 0.008*\"sentence\" + 0.008*\"year\" + 0.008*\"women\"\n",
      "Topic: 7 \n",
      "Words: 0.037*\"iran\" + 0.018*\"nuclear\" + 0.018*\"russia\" + 0.014*\"iraq\" + 0.014*\"world\" + 0.008*\"say\" + 0.007*\"china\" + 0.007*\"missile\" + 0.007*\"obama\" + 0.006*\"weapons\"\n",
      "Topic: 8 \n",
      "Words: 0.009*\"internet\" + 0.008*\"government\" + 0.007*\"right\" + 0.006*\"say\" + 0.006*\"house\" + 0.006*\"muslim\" + 0.006*\"people\" + 0.006*\"israelis\" + 0.006*\"demand\" + 0.005*\"church\"\n",
      "Topic: 9 \n",
      "Words: 0.010*\"election\" + 0.009*\"say\" + 0.009*\"drug\" + 0.009*\"party\" + 0.009*\"vote\" + 0.008*\"government\" + 0.007*\"german\" + 0.007*\"mexico\" + 0.006*\"people\" + 0.006*\"country\"\n"
     ]
    }
   ],
   "source": [
    "# For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"world\" + 0.004*\"global\" + 0.004*\"china\" + 0.004*\"food\" + 0.004*\"karadzic\" + 0.004*\"crisis\" + 0.003*\"berlusconi\" + 0.003*\"say\" + 0.003*\"israel\" + 0.003*\"rise\"\n",
      "Topic: 1 Word: 0.006*\"ahmadinejad\" + 0.005*\"china\" + 0.005*\"japan\" + 0.004*\"world\" + 0.003*\"fukushima\" + 0.003*\"people\" + 0.003*\"tsunami\" + 0.003*\"ship\" + 0.003*\"quake\" + 0.003*\"kill\"\n",
      "Topic: 2 Word: 0.006*\"zimbabwe\" + 0.005*\"police\" + 0.004*\"israel\" + 0.004*\"ossetia\" + 0.004*\"bank\" + 0.003*\"die\" + 0.003*\"somalia\" + 0.003*\"year\" + 0.003*\"india\" + 0.003*\"china\"\n",
      "Topic: 3 Word: 0.013*\"kill\" + 0.009*\"iraq\" + 0.008*\"bomb\" + 0.007*\"pakistan\" + 0.007*\"attack\" + 0.007*\"iran\" + 0.007*\"troop\" + 0.006*\"israel\" + 0.006*\"strike\" + 0.006*\"afghanistan\"\n",
      "Topic: 4 Word: 0.005*\"police\" + 0.004*\"olympic\" + 0.004*\"kill\" + 0.004*\"pope\" + 0.004*\"china\" + 0.004*\"people\" + 0.003*\"gaza\" + 0.003*\"abuse\" + 0.003*\"world\" + 0.003*\"vatican\"\n",
      "Topic: 5 Word: 0.007*\"israel\" + 0.006*\"gaza\" + 0.005*\"right\" + 0.005*\"israeli\" + 0.005*\"kill\" + 0.005*\"attack\" + 0.004*\"say\" + 0.004*\"human\" + 0.004*\"world\" + 0.004*\"arrest\"\n",
      "Topic: 6 Word: 0.010*\"georgia\" + 0.006*\"israel\" + 0.005*\"israeli\" + 0.004*\"kill\" + 0.004*\"iran\" + 0.004*\"assange\" + 0.004*\"world\" + 0.004*\"protest\" + 0.004*\"russia\" + 0.004*\"palestinians\"\n",
      "Topic: 7 Word: 0.013*\"korea\" + 0.011*\"north\" + 0.010*\"nuclear\" + 0.008*\"iran\" + 0.007*\"south\" + 0.005*\"russia\" + 0.005*\"missile\" + 0.005*\"say\" + 0.004*\"sanction\" + 0.004*\"mugabe\"\n",
      "Topic: 8 Word: 0.005*\"internet\" + 0.004*\"iran\" + 0.004*\"government\" + 0.004*\"chavez\" + 0.004*\"court\" + 0.004*\"protest\" + 0.004*\"sentence\" + 0.003*\"china\" + 0.003*\"arrest\" + 0.003*\"google\"\n",
      "Topic: 9 Word: 0.004*\"olympics\" + 0.004*\"putin\" + 0.004*\"russia\" + 0.004*\"say\" + 0.004*\"president\" + 0.004*\"sarkozy\" + 0.003*\"world\" + 0.003*\"east\" + 0.003*\"peace\" + 0.003*\"murdoch\"\n"
     ]
    }
   ],
   "source": [
    "# Running LDA using TF-IDF\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['president', 'france', 'say', 'brexit', 'donald', 'trump']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "# We will check where our test document would be classified.\n",
    "\n",
    "cleaned_headlines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8713300824165344\t \n",
      "Topic: 0.010*\"election\" + 0.009*\"say\" + 0.009*\"drug\" + 0.009*\"party\" + 0.009*\"vote\" + 0.008*\"government\" + 0.007*\"german\" + 0.007*\"mexico\" + 0.006*\"people\" + 0.006*\"country\"\n",
      "\n",
      "Score: 0.014299782924354076\t \n",
      "Topic: 0.026*\"police\" + 0.013*\"president\" + 0.013*\"russian\" + 0.009*\"russia\" + 0.009*\"protest\" + 0.008*\"woman\" + 0.008*\"protesters\" + 0.008*\"sentence\" + 0.008*\"year\" + 0.008*\"women\"\n",
      "\n",
      "Score: 0.014299423433840275\t \n",
      "Topic: 0.009*\"internet\" + 0.008*\"government\" + 0.007*\"right\" + 0.006*\"say\" + 0.006*\"house\" + 0.006*\"muslim\" + 0.006*\"people\" + 0.006*\"israelis\" + 0.006*\"demand\" + 0.005*\"church\"\n",
      "\n",
      "Score: 0.014297443442046642\t \n",
      "Topic: 0.037*\"iran\" + 0.018*\"nuclear\" + 0.018*\"russia\" + 0.014*\"iraq\" + 0.014*\"world\" + 0.008*\"say\" + 0.007*\"china\" + 0.007*\"missile\" + 0.007*\"obama\" + 0.006*\"weapons\"\n",
      "\n",
      "Score: 0.0142962746322155\t \n",
      "Topic: 0.034*\"kill\" + 0.021*\"attack\" + 0.016*\"bomb\" + 0.014*\"force\" + 0.010*\"army\" + 0.010*\"soldier\" + 0.010*\"troop\" + 0.009*\"pakistan\" + 0.008*\"taliban\" + 0.008*\"police\"\n",
      "\n",
      "Score: 0.014296191744506359\t \n",
      "Topic: 0.045*\"israel\" + 0.020*\"israeli\" + 0.018*\"china\" + 0.018*\"state\" + 0.015*\"right\" + 0.014*\"say\" + 0.014*\"human\" + 0.011*\"palestinian\" + 0.010*\"unite\" + 0.010*\"iran\"\n",
      "\n",
      "Score: 0.014295331202447414\t \n",
      "Topic: 0.018*\"bank\" + 0.013*\"government\" + 0.010*\"protest\" + 0.009*\"court\" + 0.008*\"chinese\" + 0.008*\"china\" + 0.008*\"million\" + 0.008*\"police\" + 0.006*\"people\" + 0.006*\"billion\"\n",
      "\n",
      "Score: 0.014295320957899094\t \n",
      "Topic: 0.036*\"gaza\" + 0.020*\"israel\" + 0.013*\"ship\" + 0.013*\"hamas\" + 0.009*\"israeli\" + 0.007*\"children\" + 0.006*\"say\" + 0.006*\"georgia\" + 0.006*\"sell\" + 0.006*\"report\"\n",
      "\n",
      "Score: 0.01429508812725544\t \n",
      "Topic: 0.023*\"north\" + 0.022*\"korea\" + 0.021*\"kill\" + 0.019*\"south\" + 0.015*\"pakistan\" + 0.011*\"strike\" + 0.008*\"year\" + 0.008*\"death\" + 0.008*\"children\" + 0.007*\"say\"\n",
      "\n",
      "Score: 0.014295071363449097\t \n",
      "Topic: 0.017*\"world\" + 0.010*\"india\" + 0.009*\"power\" + 0.009*\"water\" + 0.008*\"japan\" + 0.007*\"plant\" + 0.007*\"food\" + 0.007*\"global\" + 0.007*\"years\" + 0.006*\"saudi\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8713445067405701\t \n",
      "Topic: 0.005*\"police\" + 0.004*\"olympic\" + 0.004*\"kill\" + 0.004*\"pope\" + 0.004*\"china\" + 0.004*\"people\" + 0.003*\"gaza\" + 0.003*\"abuse\" + 0.003*\"world\" + 0.003*\"vatican\"\n",
      "\n",
      "Score: 0.014296788722276688\t \n",
      "Topic: 0.004*\"olympics\" + 0.004*\"putin\" + 0.004*\"russia\" + 0.004*\"say\" + 0.004*\"president\" + 0.004*\"sarkozy\" + 0.003*\"world\" + 0.003*\"east\" + 0.003*\"peace\" + 0.003*\"murdoch\"\n",
      "\n",
      "Score: 0.01429586112499237\t \n",
      "Topic: 0.013*\"korea\" + 0.011*\"north\" + 0.010*\"nuclear\" + 0.008*\"iran\" + 0.007*\"south\" + 0.005*\"russia\" + 0.005*\"missile\" + 0.005*\"say\" + 0.004*\"sanction\" + 0.004*\"mugabe\"\n",
      "\n",
      "Score: 0.014295285567641258\t \n",
      "Topic: 0.005*\"internet\" + 0.004*\"iran\" + 0.004*\"government\" + 0.004*\"chavez\" + 0.004*\"court\" + 0.004*\"protest\" + 0.004*\"sentence\" + 0.003*\"china\" + 0.003*\"arrest\" + 0.003*\"google\"\n",
      "\n",
      "Score: 0.014294859021902084\t \n",
      "Topic: 0.010*\"georgia\" + 0.006*\"israel\" + 0.005*\"israeli\" + 0.004*\"kill\" + 0.004*\"iran\" + 0.004*\"assange\" + 0.004*\"world\" + 0.004*\"protest\" + 0.004*\"russia\" + 0.004*\"palestinians\"\n",
      "\n",
      "Score: 0.014294830150902271\t \n",
      "Topic: 0.005*\"world\" + 0.004*\"global\" + 0.004*\"china\" + 0.004*\"food\" + 0.004*\"karadzic\" + 0.004*\"crisis\" + 0.003*\"berlusconi\" + 0.003*\"say\" + 0.003*\"israel\" + 0.003*\"rise\"\n",
      "\n",
      "Score: 0.014294739812612534\t \n",
      "Topic: 0.007*\"israel\" + 0.006*\"gaza\" + 0.005*\"right\" + 0.005*\"israeli\" + 0.005*\"kill\" + 0.005*\"attack\" + 0.004*\"say\" + 0.004*\"human\" + 0.004*\"world\" + 0.004*\"arrest\"\n",
      "\n",
      "Score: 0.014294495806097984\t \n",
      "Topic: 0.006*\"ahmadinejad\" + 0.005*\"china\" + 0.005*\"japan\" + 0.004*\"world\" + 0.003*\"fukushima\" + 0.003*\"people\" + 0.003*\"tsunami\" + 0.003*\"ship\" + 0.003*\"quake\" + 0.003*\"kill\"\n",
      "\n",
      "Score: 0.014294435270130634\t \n",
      "Topic: 0.013*\"kill\" + 0.009*\"iraq\" + 0.008*\"bomb\" + 0.007*\"pakistan\" + 0.007*\"attack\" + 0.007*\"iran\" + 0.007*\"troop\" + 0.006*\"israel\" + 0.006*\"strike\" + 0.006*\"afghanistan\"\n",
      "\n",
      "Score: 0.014294245280325413\t \n",
      "Topic: 0.006*\"zimbabwe\" + 0.005*\"police\" + 0.004*\"israel\" + 0.004*\"ossetia\" + 0.004*\"bank\" + 0.003*\"die\" + 0.003*\"somalia\" + 0.003*\"year\" + 0.003*\"india\" + 0.003*\"china\"\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF model.\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6666535139083862\t Topic: 0.037*\"iran\" + 0.018*\"nuclear\" + 0.018*\"russia\" + 0.014*\"iraq\" + 0.014*\"world\"\n",
      "Score: 0.19996194541454315\t Topic: 0.009*\"internet\" + 0.008*\"government\" + 0.007*\"right\" + 0.006*\"say\" + 0.006*\"house\"\n",
      "Score: 0.016678424552083015\t Topic: 0.018*\"bank\" + 0.013*\"government\" + 0.010*\"protest\" + 0.009*\"court\" + 0.008*\"chinese\"\n",
      "Score: 0.01667626015841961\t Topic: 0.017*\"world\" + 0.010*\"india\" + 0.009*\"power\" + 0.009*\"water\" + 0.008*\"japan\"\n",
      "Score: 0.016672149300575256\t Topic: 0.023*\"north\" + 0.022*\"korea\" + 0.021*\"kill\" + 0.019*\"south\" + 0.015*\"pakistan\"\n",
      "Score: 0.016672130674123764\t Topic: 0.036*\"gaza\" + 0.020*\"israel\" + 0.013*\"ship\" + 0.013*\"hamas\" + 0.009*\"israeli\"\n",
      "Score: 0.016672059893608093\t Topic: 0.026*\"police\" + 0.013*\"president\" + 0.013*\"russian\" + 0.009*\"russia\" + 0.009*\"protest\"\n",
      "Score: 0.016671735793352127\t Topic: 0.010*\"election\" + 0.009*\"say\" + 0.009*\"drug\" + 0.009*\"party\" + 0.009*\"vote\"\n",
      "Score: 0.01667146384716034\t Topic: 0.045*\"israel\" + 0.020*\"israeli\" + 0.018*\"china\" + 0.018*\"state\" + 0.015*\"right\"\n",
      "Score: 0.016670336946845055\t Topic: 0.034*\"kill\" + 0.021*\"attack\" + 0.016*\"bomb\" + 0.014*\"force\" + 0.010*\"army\"\n"
     ]
    }
   ],
   "source": [
    "# Testing model on unseen document\n",
    "\n",
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "# lda_model[bow_vector] # Gets topic probabilities for unseen document \n",
    "# # (7, 0.66596633) is the highest score\n",
    "\n",
    "# for topic in lda_model[bow_vector]:\n",
    "#     print(topic)\n",
    "\n",
    "# lda_model.print_topic(index, 7)\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(temp_file)\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda.gz']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(lda_model, \"lda.gz\")\n",
    "# LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
