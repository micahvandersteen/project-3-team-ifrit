{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline topic analysis with Non-negative Matrix Factorization (NMF) \n",
    "\n",
    "The purpose of this analysis is to find dominant topics across news headlines (25 per day, over 1 year). These topics will later be correlated with daily stock market loss/gain information to understand how certain topics may influence the stock market.\n",
    "\n",
    "## What is NMF?\n",
    "\n",
    "Non-negative Matrix Factorization is a mathematical technique that when applied to documents can take data with many features (e.g. 1000s of topics) and convert them into a smaller set of topics. It is similar to LDA in that it is a way to discover higher-level topics out of individual words present in any set of document (in our case, news headlines). You can use NMF to get a sense of the overall themes in a set of documents.\n",
    "\n",
    "NMF is an unsupervised machine learning model that works by taking your matrix _A_ of documents x words (e.g. 50 documents and 5000 words) and returning topics (_W_) and weights/coefficients (_H_) for the topics.\n",
    "\n",
    "As Rob Salgado explains in his [excellent article on NMF](https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45), \"NMF will modify the initial values of W and H so that the product approaches A until either the approximation error converges or the max iterations are reached.\" \n",
    "\n",
    "Like LDA, the \"topics\" that NMF finds aren't specific words (e.g., \"This headline is about 'war'\") but instead conceptually similar groups of words that together make up a theme (e.g., \"This headline is similar to the words 'war', 'crisis', 'iran'...\").\n",
    "\n",
    "Once you've created your NMF model, you can feed in a document and the model will score the overall relevancy of your document against the main _x_ topics found in your overall corpus. In other words, it will tell you which of the main topics found in the overall corpus are also found in your document, and to what extent.\n",
    "\n",
    "## Why NMF?\n",
    "\n",
    "NMF is regarded to be superior to LDA in terms of efficiency and accuracy, though it does not appear to be as popular. \n",
    "\n",
    "## How does NMF perform compared to LDA?\n",
    "\n",
    "More here later.\n",
    "\n",
    "## Credit\n",
    "\n",
    "Parts of this work borrow from Rob Salgado's [excellent NMF tutorial](https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45), as well as Ravish Chawla's [NMF tutorial](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/stacy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"../Data/RedditNews.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News\n",
       "0  A 117-year-old woman in Mexico City finally re...\n",
       "1   IMF chief backs Athens as permanent Olympic host\n",
       "2  The president of France says if Brexit won, so...\n",
       "3  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting just the headlines for our corpus\n",
    "headlines = data[['News']]\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lemmitize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v') # pos='v' means it peforms stemming with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and words shorter than 3 characters, then lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['The', 'president', 'of', 'France', 'says', 'if', 'Brexit', 'won,', 'so', 'can', 'Donald', 'Trump']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['president', 'france', 'say', 'brexit', 'donald', 'trump']\n"
     ]
    }
   ],
   "source": [
    "sample = headlines['News'][2]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['cleaned_headlines']  = headlines['News'].map(preprocess)\n",
    "headlines['cleaned_headlines'][:5] # Check the results\n",
    "\n",
    "train, test = train_test_split(headlines['cleaned_headlines'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41624    [portugal, raid, pension, fund, meet, deficit,...\n",
       "5620     [fearless, father, throw, suicide, bomber, sav...\n",
       "65161    [reddit, spend, morning, write, brief, history...\n",
       "72850              [vote, force, isps, disconnect, pirate]\n",
       "59075    [chvez, order, jet, intercept, military, plane...\n",
       "                               ...                        \n",
       "68615                                [england, households]\n",
       "70154              [iran, hold, american, student, prison]\n",
       "55963    [turkey, position, missiles, repulse, israeli,...\n",
       "23059    [china, moon, rover, activate, science, tool, ...\n",
       "64850         [spanish, intelligence, agents, expel, cuba]\n",
       "Name: cleaned_headlines, Length: 7361, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and transform the text using TF-IDF\n",
    "Sentence here about why we use TF-IDF in this case and not BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the headlines using TF-IDF to create features to train our model on\n",
    "texts = train\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "\n",
    "'''\n",
    "Filter out irrelevant words:\n",
    "Keep tokens that appear in at least 15 documents\n",
    "Keep only the 100,000 most frequent tokens\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, keep_n=100000)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3, # ignore words that appear in less than 3 articles\n",
    "    max_df=0.85, # ignore words that appear in more than 85% of articles\n",
    "    max_features=5000, # limit the number of important words to 5,000\n",
    "    ngram_range=(1, 2), # look for both words and two-word phrases\n",
    "    preprocessor=' '.join # join the tokenized words instead of creating a list\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit the NMF model on our headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=10, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create NMF model and fit it\n",
    "# We are using ten topic groupings here so it is directly comparable to the LDA model's performance\n",
    "\n",
    "'''\n",
    "\"Nonnegative Double Singular Value Decomposition (NNDSVD) is a new method designed to enhance the \n",
    "initialization stage of the nonnegative matrix factorization.\n",
    "\n",
    "\"NNDSVD is well suited to initialize NMF algorithms with sparse factors.\"\" - http://nimfa.biolab.si/nimfa.methods.seeding.nndsvd.html\n",
    "'''\n",
    "model = NMF(n_components=10, init='nndsvd')\n",
    "model.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 1</th>\n",
       "      <th>Topic # 2</th>\n",
       "      <th>Topic # 3</th>\n",
       "      <th>Topic # 4</th>\n",
       "      <th>Topic # 5</th>\n",
       "      <th>Topic # 6</th>\n",
       "      <th>Topic # 7</th>\n",
       "      <th>Topic # 8</th>\n",
       "      <th>Topic # 9</th>\n",
       "      <th>Topic #10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police</td>\n",
       "      <td>gaza</td>\n",
       "      <td>isis</td>\n",
       "      <td>korea</td>\n",
       "      <td>ukraine</td>\n",
       "      <td>wikileaks</td>\n",
       "      <td>say</td>\n",
       "      <td>snowden</td>\n",
       "      <td>libya</td>\n",
       "      <td>georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iran</td>\n",
       "      <td>israel</td>\n",
       "      <td>islamic state</td>\n",
       "      <td>north</td>\n",
       "      <td>russia</td>\n",
       "      <td>assange</td>\n",
       "      <td>china</td>\n",
       "      <td>edward</td>\n",
       "      <td>egypt</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kill</td>\n",
       "      <td>israeli</td>\n",
       "      <td>islamic</td>\n",
       "      <td>north korea</td>\n",
       "      <td>russian</td>\n",
       "      <td>julian</td>\n",
       "      <td>world</td>\n",
       "      <td>edward snowden</td>\n",
       "      <td>protest</td>\n",
       "      <td>ossetia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>hamas</td>\n",
       "      <td>ebola</td>\n",
       "      <td>south</td>\n",
       "      <td>crimea</td>\n",
       "      <td>julian assange</td>\n",
       "      <td>climate</td>\n",
       "      <td>spy</td>\n",
       "      <td>protesters</td>\n",
       "      <td>georgian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>palestinians</td>\n",
       "      <td>state</td>\n",
       "      <td>korean</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>cable</td>\n",
       "      <td>change</td>\n",
       "      <td>surveillance</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>south ossetia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>gadhafi</td>\n",
       "      <td>factor</td>\n",
       "      <td>libyans</td>\n",
       "      <td>jail years</td>\n",
       "      <td>latest</td>\n",
       "      <td>mark</td>\n",
       "      <td>onion</td>\n",
       "      <td>methane</td>\n",
       "      <td>humans</td>\n",
       "      <td>sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>gaddafi</td>\n",
       "      <td>shouldn</td>\n",
       "      <td>libyan rebel</td>\n",
       "      <td>receive</td>\n",
       "      <td>later</td>\n",
       "      <td>marine</td>\n",
       "      <td>depth</td>\n",
       "      <td>mexican police</td>\n",
       "      <td>surface</td>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>fukushima plant</td>\n",
       "      <td>factory</td>\n",
       "      <td>libyan</td>\n",
       "      <td>elites</td>\n",
       "      <td>lash</td>\n",
       "      <td>marijuana</td>\n",
       "      <td>omar</td>\n",
       "      <td>research</td>\n",
       "      <td>human shield</td>\n",
       "      <td>historic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>fukushima nuclear</td>\n",
       "      <td>shots</td>\n",
       "      <td>libya</td>\n",
       "      <td>record number</td>\n",
       "      <td>larger</td>\n",
       "      <td>marathon</td>\n",
       "      <td>describe</td>\n",
       "      <td>resistant</td>\n",
       "      <td>surveillance</td>\n",
       "      <td>hire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>fukushima daiichi</td>\n",
       "      <td>mideast</td>\n",
       "      <td>libel</td>\n",
       "      <td>embargo</td>\n",
       "      <td>large scale</td>\n",
       "      <td>map</td>\n",
       "      <td>deserve</td>\n",
       "      <td>mexico drug</td>\n",
       "      <td>hottest</td>\n",
       "      <td>sanctuary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4990 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Topic # 1     Topic # 2      Topic # 3      Topic # 4  \\\n",
       "0                police          gaza           isis          korea   \n",
       "1                  iran        israel  islamic state          north   \n",
       "2                  kill       israeli        islamic    north korea   \n",
       "3                people         hamas          ebola          south   \n",
       "4           afghanistan  palestinians          state         korean   \n",
       "...                 ...           ...            ...            ...   \n",
       "4985            gadhafi        factor        libyans     jail years   \n",
       "4986            gaddafi       shouldn   libyan rebel        receive   \n",
       "4987    fukushima plant       factory         libyan         elites   \n",
       "4988  fukushima nuclear         shots          libya  record number   \n",
       "4989  fukushima daiichi       mideast          libel        embargo   \n",
       "\n",
       "        Topic # 5       Topic # 6 Topic # 7       Topic # 8     Topic # 9  \\\n",
       "0         ukraine       wikileaks       say         snowden         libya   \n",
       "1          russia         assange     china          edward         egypt   \n",
       "2         russian          julian     world  edward snowden       protest   \n",
       "3          crimea  julian assange   climate             spy    protesters   \n",
       "4       ukrainian           cable    change    surveillance      egyptian   \n",
       "...           ...             ...       ...             ...           ...   \n",
       "4985       latest            mark     onion         methane        humans   \n",
       "4986        later          marine     depth  mexican police       surface   \n",
       "4987         lash       marijuana      omar        research  human shield   \n",
       "4988       larger        marathon  describe       resistant  surveillance   \n",
       "4989  large scale             map   deserve     mexico drug       hottest   \n",
       "\n",
       "          Topic #10  \n",
       "0           georgia  \n",
       "1            russia  \n",
       "2           ossetia  \n",
       "3          georgian  \n",
       "4     south ossetia  \n",
       "...             ...  \n",
       "4985         sample  \n",
       "4986        samsung  \n",
       "4987       historic  \n",
       "4988           hire  \n",
       "4989      sanctuary  \n",
       "\n",
       "[4990 rows x 10 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the topics to visually inspect them\n",
    "\n",
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(10):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:10 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic #' + '{:2d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "get_nmf_topics(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, these topics seem to be a lot more coherent than what the LDA model produced. There's not only better in-topic coherence (i.e. the words relate to each other well) but also distinctions between topics (i.e. there's less keyword overlap from topic to topic).\n",
    "\n",
    "## Testing the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>pred_topic_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[portugal, raid, pension, fund, meet, deficit,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fearless, father, throw, suicide, bomber, sav...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[reddit, spend, morning, write, brief, history...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[vote, force, isps, disconnect, pirate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[chvez, order, jet, intercept, military, plane...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                test  pred_topic_num\n",
       "0  [portugal, raid, pension, fund, meet, deficit,...               1\n",
       "1  [fearless, father, throw, suicide, bomber, sav...               4\n",
       "2  [reddit, spend, morning, write, brief, history...               1\n",
       "3            [vote, force, isps, disconnect, pirate]               1\n",
       "4  [chvez, order, jet, intercept, military, plane...               5"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# See how well our model performed by using the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(test)\n",
    "X_test = model.transform(tfidf_test)\n",
    "\n",
    "# Get the top predicted topic\n",
    "predicted_topics = [np.argsort(each)[::-1][0] + 1 for each in X_test]    \n",
    "\n",
    "# Add to the df\n",
    "df['test'] = test\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['pred_topic_num'] = predicted_topics\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a single topic per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the headlines into topics isn't working so well against the test data. Maybe more topics are better? This is where a **coherence score** would come in: it is a score that tells you how \"coherent\" (closely related) the words within a topic are, and you can use it to [automatically select the best number of topics](https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45) to train your model on. That is currently beyond the scope of this project, but will be the next area for exploration.\n",
    "\n",
    "Since we know that we want to find one topic per day to feed into other models, we'll now take our trained model and set it loose on the combined headlines from each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "      <th>combined_headlines</th>\n",
       "      <th>daily_topic</th>\n",
       "      <th>daily_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
       "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
       "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
       "      <td>b'Russian tanks are moving towards the capital...</td>\n",
       "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
       "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
       "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
       "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'This is a busy day:  The European Union has ...</td>\n",
       "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
       "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
       "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
       "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
       "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
       "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
       "      <td>Georgia 'downs two Russian warplanes' as coun...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
       "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
       "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
       "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
       "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
       "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
       "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
       "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'China to overtake US as largest manufacturer'</td>\n",
       "      <td>b'War in South Ossetia [PICS]'</td>\n",
       "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
       "      <td>b' Russia has just beaten the United States ov...</td>\n",
       "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
       "      <td>b'Russia is so much better at war'</td>\n",
       "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
       "      <td>Why wont America and Nato help us? If they wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
       "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
       "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
       "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
       "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
       "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
       "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
       "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
       "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
       "      <td>b'War in Georgia: The Israeli connection'</td>\n",
       "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
       "      <td>b'Christopher King argues that the US and NATO...</td>\n",
       "      <td>b'America: The New Mexico?'</td>\n",
       "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
       "      <td>Remember that adorable 9-year-old who sang at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
       "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
       "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
       "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
       "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
       "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
       "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
       "      <td>b'Russian forces sink Georgian ships '</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
       "      <td>b'Israeli defence minister: US against strike ...</td>\n",
       "      <td>b'Gorbachev: We Had No Choice'</td>\n",
       "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
       "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
       "      <td>b'Georgian president  says US military will ta...</td>\n",
       "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
       "      <td>U.S. refuses Israel weapons to attack Iran: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>b'All the experts admit that we should legalis...</td>\n",
       "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
       "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
       "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
       "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
       "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
       "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
       "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
       "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
       "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
       "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
       "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
       "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
       "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
       "      <td>All the experts admit that we should legalise...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
       "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
       "2  2008-08-12      0  b'Remember that adorable 9-year-old who sang a...   \n",
       "3  2008-08-13      0  b' U.S. refuses Israel weapons to attack Iran:...   \n",
       "4  2008-08-14      1  b'All the experts admit that we should legalis...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0            b'BREAKING: Musharraf to be impeached.'   \n",
       "1        b'Bush puts foot down on Georgian conflict'   \n",
       "2                 b\"Russia 'ends Georgia operation'\"   \n",
       "3  b\"When the president ordered to attack Tskhinv...   \n",
       "4  b'War in South Osetia - 89 pictures made by a ...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b'Russia Today: Columns of troops roll into So...   \n",
       "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
       "2  b'\"If we had no sexual harassment we would hav...   \n",
       "3  b' Israel clears troops who killed Reuters cam...   \n",
       "4  b'Swedish wrestler Ara Abrahamian throws away ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b'Russian tanks are moving towards the capital...   \n",
       "1  b'Georgian army flees in disarray as Russians ...   \n",
       "2  b\"Al-Qa'eda is losing support in Iraq because ...   \n",
       "3  b'Britain\\'s policy of being tough on drugs is...   \n",
       "4  b'Russia exaggerated the death toll in South O...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
       "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
       "2  b'Ceasefire in Georgia: Putin Outmaneuvers the...   \n",
       "3  b'Body of 14 year old found in trunk; Latest (...   \n",
       "4  b'Missile That Killed 9 Inside Pakistan May Ha...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b'150 Russian tanks have entered South Ossetia...   \n",
       "1  b'What were the Mossad with fraudulent New Zea...   \n",
       "2  b'Why Microsoft and Intel tried to kill the XO...   \n",
       "3  b'China has moved 10 *million* quake survivors...   \n",
       "4  b\"Rushdie Condemns Random House's Refusal to P...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
       "1  b'Russia angered by Israeli military sale to G...   \n",
       "2  b'Stratfor: The Russo-Georgian War and the Bal...   \n",
       "3  b\"Bush announces Operation Get All Up In Russi...   \n",
       "4  b'Poland and US agree to missle defense deal. ...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  b\"The 'enemy combatent' trials are nothing but...  ...   \n",
       "1  b'An American citizen living in S.Ossetia blam...  ...   \n",
       "2  b\"I'm Trying to Get a Sense of This Whole Geor...  ...   \n",
       "3             b'Russian forces sink Georgian ships '  ...   \n",
       "4  b'Will the Russians conquer Tblisi? Bet on it,...  ...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b'This is a busy day:  The European Union has ...   \n",
       "1    b'China to overtake US as largest manufacturer'   \n",
       "2         b'Russia, Georgia, and NATO: Cold War Two'   \n",
       "3  b'Russian convoy heads into Georgia, violating...   \n",
       "4  b'\"Non-media\" photos of South Ossetia/Georgia ...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
       "1                     b'War in South Ossetia [PICS]'   \n",
       "2  b'Remember that adorable 62-year-old who led y...   \n",
       "3  b'Israeli defence minister: US against strike ...   \n",
       "4  b'Georgian TV reporter shot by Russian sniper ...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
       "1  b'Israeli Physicians Group Condemns State Tort...   \n",
       "2          b'War in Georgia: The Israeli connection'   \n",
       "3                     b'Gorbachev: We Had No Choice'   \n",
       "4  b'Saudi Arabia: Mother moves to block child ma...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
       "1  b' Russia has just beaten the United States ov...   \n",
       "2  b'All signs point to the US encouraging Georgi...   \n",
       "3  b'Witness: Russian forces head towards Tbilisi...   \n",
       "4   b'Taliban wages war on humanitarian aid workers'   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b'Indian shoe manufactory  - And again in a se...   \n",
       "1  b'Perhaps *the* question about the Georgia - R...   \n",
       "2  b'Christopher King argues that the US and NATO...   \n",
       "3  b' Quarter of Russians blame U.S. for conflict...   \n",
       "4  b'Russia: World  \"can forget about\" Georgia\\'s...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
       "1                 b'Russia is so much better at war'   \n",
       "2                        b'America: The New Mexico?'   \n",
       "3  b'Georgian president  says US military will ta...   \n",
       "4  b'Darfur rebels accuse Sudan of mounting major...   \n",
       "\n",
       "                                               Top25  \\\n",
       "0           b\"No Help for Mexico's Kidnapping Surge\"   \n",
       "1  b\"So this is what it's come to: trading sex fo...   \n",
       "2  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...   \n",
       "3  b'2006: Nobel laureate Aleksander Solzhenitsyn...   \n",
       "4  b'Philippines : Peace Advocate say Muslims nee...   \n",
       "\n",
       "                                  combined_headlines daily_topic  \\\n",
       "0   Georgia 'downs two Russian warplanes' as coun...           1   \n",
       "1   Why wont America and Nato help us? If they wo...           1   \n",
       "2   Remember that adorable 9-year-old who sang at...           1   \n",
       "3    U.S. refuses Israel weapons to attack Iran: ...           1   \n",
       "4   All the experts admit that we should legalise...           1   \n",
       "\n",
       "                                         daily_words  \n",
       "0  0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...  \n",
       "1  0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...  \n",
       "2  0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...  \n",
       "3  0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...  \n",
       "4  0.006*\"russian\" + 0.006*\"russia\" + 0.004*\"forc...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_topic = pd.read_csv(\"../Data/Combined_News_DJIA_single_topic.csv\")\n",
    "single_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenize headlines \n",
    "single_topic['tokenized'] = single_topic['combined_headlines'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the headlines using TF-IDF to create features to train our model on\n",
    "dictionary = gensim.corpora.Dictionary(single_topic['tokenized'])\n",
    "\n",
    "'''\n",
    "Filter out irrelevant words:\n",
    "Keep tokens that appear in at least 15 documents\n",
    "Keep only the 100,000 most frequent tokens\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, keep_n=100000)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3, # ignore words that appear in less than 3 articles\n",
    "    max_df=0.85, # ignore words that appear in more than 85% of articles\n",
    "    max_features=5000, # limit the number of important words to 5,000\n",
    "    ngram_range=(1, 2), # look for both words and two-word phrases\n",
    "    preprocessor=' '.join # join the tokenized words instead of creating a list\n",
    ")\n",
    "\n",
    "# Helped by https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "doctermmatrix = tfidf_vectorizer.fit_transform(single_topic['tokenized'])\n",
    "\n",
    "topic_values = model.transform(doctermmatrix)\n",
    "single_topic['nmf_topic'] = topic_values.argmax(axis=1)\n",
    "single_topic.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create human readable topic word lists to append to df\n",
    "nmf_topics = []\n",
    "\n",
    "for i,topic in enumerate(model.components_):\n",
    "    nmf_topics.append([tfidf_vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    \n",
    "def get_topic_list(topic):\n",
    "    return nmf_topics[topic]\n",
    "    \n",
    "single_topic['nmf_topic_readable'] = single_topic['nmf_topic'].apply(get_topic_list)\n",
    "\n",
    "single_topic.rename(columns={\n",
    "    'daily_topic': 'lda_topic',\n",
    "    'daily_words': 'lda_topic_readable'\n",
    "}, inplace=True)\n",
    "\n",
    "single_topic.drop(labels=['tokenized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated data\n",
    "single_topic.to_csv(\"../Data/Combined_News_DJIA_single_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
