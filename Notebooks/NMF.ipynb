{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline topic analysis with Non-negative Matrix Factorization (NMF) \n",
    "\n",
    "The purpose of this analysis is to find dominant topics across news headlines (25 per day, over 1 year). These topics will later be correlated with daily stock market loss/gain information to understand how certain topics may influence the stock market.\n",
    "\n",
    "## What is NMF?\n",
    "\n",
    "Latent Dirichlet allocation is a way to discover higher-level topics out of individual words present in any set of document (in our case, news headlines). You can use LDA to get a sense of the overall themes in a set of documents.\n",
    "\n",
    "LDA is an unsupervised machine learning model that works by analyzing two things: a distribution of topics in a document, and a distribution of words in a topic. The \"topics\" it finds aren't specific words (e.g., \"This headline is about 'war'\") but instead conceptually similar groups of words that together make up a theme (e.g., \"This headline is similar to the words 'war', 'crisis', 'iran'...\").\n",
    "\n",
    "Once you've created your LDA model, you can feed in a document and the model will score the overall relevancy of your document against the main x topics found in your overall corpus. In other words, it will tell you which of the main topics found in the overall corpus are also found in your document, and to what extent.\n",
    "\n",
    "## Why NMF?\n",
    "\n",
    "LDA is a fairly popular topic modelling choice among NLP professionals, and relatively straightforward to implement. Using LDA, we were able to find 10 dominant themes in eight years' worth of news headlines within about an hourâ€”a task that would take a human days to analyze.\n",
    "\n",
    "## How does NMF perform compared to LDA?\n",
    "\n",
    "More here later.\n",
    "\n",
    "## Credit\n",
    "\n",
    "Parts of this work borrow from Ravish Chawla's [NMF tutorial](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/stacy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "# import sklearn\n",
    "# import sys\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "# import pickle\n",
    "\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"../Data/RedditNews.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News\n",
       "0  A 117-year-old woman in Mexico City finally re...\n",
       "1   IMF chief backs Athens as permanent Olympic host\n",
       "2  The president of France says if Brexit won, so...\n",
       "3  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting just the headlines for our corpus\n",
    "headlines = data[['News']]\n",
    "del data\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lemmitize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v') # pos='v' means it peforms stemming with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and words shorter than 3 characters, then lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['The', 'president', 'of', 'France', 'says', 'if', 'Brexit', 'won,', 'so', 'can', 'Donald', 'Trump']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['president', 'france', 'say', 'brexit', 'donald', 'trump']\n"
     ]
    }
   ],
   "source": [
    "sample = headlines['News'][2]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [year, woman, mexico, city, finally, receive, ...\n",
       "1      [chief, back, athens, permanent, olympic, host]\n",
       "2      [president, france, say, brexit, donald, trump]\n",
       "3    [british, police, hours, notice, threaten, hun...\n",
       "4    [nobel, laureates, urge, greenpeace, stop, opp...\n",
       "Name: News, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_headlines = headlines['News'].map(preprocess)\n",
    "cleaned_headlines[:5] # Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the cleaned_headlines lists to strings, so we can feed to CountVectorizer\n",
    "headlines['cleaned_headlines'] = [' '.join(headline) for headline in cleaned_headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        year woman mexico city finally receive birth c...\n",
       "1                 chief back athens permanent olympic host\n",
       "2                 president france say brexit donald trump\n",
       "3        british police hours notice threaten hunger st...\n",
       "4         nobel laureates urge greenpeace stop oppose gmos\n",
       "                               ...                        \n",
       "73603         go berzerk akihabara stab nearby dead injure\n",
       "73604    threat world aid pandemic heterosexuals report...\n",
       "73605    angst ankara turkey steer dangerous identity c...\n",
       "73606    identity card people children database identif...\n",
       "73607    marriage say reduce status commercial transact...\n",
       "Name: cleaned_headlines, Length: 73608, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines['cleaned_headlines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "x_counts = vectorizer.fit_transform(headlines['cleaned_headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the x_counts with TF-IDF\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(x_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=10, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/models/nmf.html\n",
    "\n",
    "# Create NMF model and fit it\n",
    "model = NMF(n_components=10, init='nndsvd')\n",
    "model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 1</th>\n",
       "      <th>Topic # 2</th>\n",
       "      <th>Topic # 3</th>\n",
       "      <th>Topic # 4</th>\n",
       "      <th>Topic # 5</th>\n",
       "      <th>Topic # 6</th>\n",
       "      <th>Topic # 7</th>\n",
       "      <th>Topic # 8</th>\n",
       "      <th>Topic # 9</th>\n",
       "      <th>Topic #10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kill</td>\n",
       "      <td>korea</td>\n",
       "      <td>israel</td>\n",
       "      <td>russia</td>\n",
       "      <td>china</td>\n",
       "      <td>iran</td>\n",
       "      <td>police</td>\n",
       "      <td>world</td>\n",
       "      <td>attack</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strike</td>\n",
       "      <td>north</td>\n",
       "      <td>gaza</td>\n",
       "      <td>ukraine</td>\n",
       "      <td>japan</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>protest</td>\n",
       "      <td>news</td>\n",
       "      <td>pakistan</td>\n",
       "      <td>syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bomb</td>\n",
       "      <td>south</td>\n",
       "      <td>israeli</td>\n",
       "      <td>putin</td>\n",
       "      <td>chinese</td>\n",
       "      <td>sanction</td>\n",
       "      <td>arrest</td>\n",
       "      <td>bank</td>\n",
       "      <td>dead</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pakistan</td>\n",
       "      <td>korean</td>\n",
       "      <td>hamas</td>\n",
       "      <td>syria</td>\n",
       "      <td>india</td>\n",
       "      <td>strike</td>\n",
       "      <td>year</td>\n",
       "      <td>countries</td>\n",
       "      <td>terror</td>\n",
       "      <td>minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>palestinian</td>\n",
       "      <td>georgia</td>\n",
       "      <td>build</td>\n",
       "      <td>talk</td>\n",
       "      <td>government</td>\n",
       "      <td>largest</td>\n",
       "      <td>paris</td>\n",
       "      <td>russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soldier</td>\n",
       "      <td>jong</td>\n",
       "      <td>palestinians</td>\n",
       "      <td>russian</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>deal</td>\n",
       "      <td>dead</td>\n",
       "      <td>population</td>\n",
       "      <td>syria</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drone</td>\n",
       "      <td>launch</td>\n",
       "      <td>rocket</td>\n",
       "      <td>warn</td>\n",
       "      <td>south</td>\n",
       "      <td>iranian</td>\n",
       "      <td>years</td>\n",
       "      <td>australia</td>\n",
       "      <td>mumbai</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>civilians</td>\n",
       "      <td>test</td>\n",
       "      <td>bank</td>\n",
       "      <td>nato</td>\n",
       "      <td>warn</td>\n",
       "      <td>iraq</td>\n",
       "      <td>india</td>\n",
       "      <td>billion</td>\n",
       "      <td>taliban</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>afghan</td>\n",
       "      <td>threaten</td>\n",
       "      <td>west</td>\n",
       "      <td>military</td>\n",
       "      <td>ban</td>\n",
       "      <td>israeli</td>\n",
       "      <td>drug</td>\n",
       "      <td>biggest</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>official</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>children</td>\n",
       "      <td>missile</td>\n",
       "      <td>report</td>\n",
       "      <td>sanction</td>\n",
       "      <td>time</td>\n",
       "      <td>weapons</td>\n",
       "      <td>people</td>\n",
       "      <td>change</td>\n",
       "      <td>plan</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic # 1 Topic # 2     Topic # 3 Topic # 4   Topic # 5 Topic # 6  \\\n",
       "0       kill     korea        israel    russia       china      iran   \n",
       "1     strike     north          gaza   ukraine       japan   nuclear   \n",
       "2       bomb     south       israeli     putin     chinese  sanction   \n",
       "3   pakistan    korean         hamas     syria       india    strike   \n",
       "4     people   nuclear   palestinian   georgia       build      talk   \n",
       "5    soldier      jong  palestinians   russian  earthquake      deal   \n",
       "6      drone    launch        rocket      warn       south   iranian   \n",
       "7  civilians      test          bank      nato        warn      iraq   \n",
       "8     afghan  threaten          west  military         ban   israeli   \n",
       "9   children   missile        report  sanction        time   weapons   \n",
       "\n",
       "    Topic # 7   Topic # 8  Topic # 9  Topic #10  \n",
       "0      police       world     attack        say  \n",
       "1     protest        news   pakistan      syria  \n",
       "2      arrest        bank       dead  president  \n",
       "3        year   countries     terror   minister  \n",
       "4  government     largest      paris    russian  \n",
       "5        dead  population      syria      state  \n",
       "6       years   australia     mumbai       iraq  \n",
       "7       india     billion    taliban     turkey  \n",
       "8        drug     biggest  terrorist   official  \n",
       "9      people      change       plan     report  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(10):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic #' + '{:2d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "get_nmf_topics(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NMF' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0858615db83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleaned_headlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_headlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NMF' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test the model on a headline we already know\n",
    "cleaned_headlines[2]\n",
    "\n",
    "model(cleaned_headlines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
