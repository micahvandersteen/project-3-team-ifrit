{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline topic analysis with Non-negative Matrix Factorization (NMF) \n",
    "\n",
    "The purpose of this analysis is to find dominant topics across news headlines (25 per day, over 1 year). These topics will later be correlated with daily stock market loss/gain information to understand how certain topics may influence the stock market.\n",
    "\n",
    "## What is NMF?\n",
    "\n",
    "Non-negative Matrix Factorization is a mathematical technique that when applied to documents can take data with many features (e.g. 1000s of topics) and convert them into a smaller set of topics. It is similar to LDA in that it is a way to discover higher-level topics out of individual words present in any set of document (in our case, news headlines). You can use NMF to get a sense of the overall themes in a set of documents.\n",
    "\n",
    "NMF is an unsupervised machine learning model that works by taking your matrix _A_ of documents x words (e.g. 50 documents and 5000 words) and returning topics (_W_) and weights/coefficients (_H_) for the topics.\n",
    "\n",
    "As Rob Salgado explains in his [excellent article on NMF](https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45), \"NMF will modify the initial values of W and H so that the product approaches A until either the approximation error converges or the max iterations are reached.\" \n",
    "\n",
    "Like LDA, the \"topics\" that NMF finds aren't specific words (e.g., \"This headline is about 'war'\") but instead conceptually similar groups of words that together make up a theme (e.g., \"This headline is similar to the words 'war', 'crisis', 'iran'...\").\n",
    "\n",
    "Once you've created your NMF model, you can feed in a document and the model will score the overall relevancy of your document against the main _x_ topics found in your overall corpus. In other words, it will tell you which of the main topics found in the overall corpus are also found in your document, and to what extent.\n",
    "\n",
    "## Why NMF?\n",
    "\n",
    "NMF is regarded to be superior to LDA in terms of efficiency and accuracy, though it does not appear to be as popular. \n",
    "\n",
    "## How does NMF perform compared to LDA?\n",
    "\n",
    "More here later.\n",
    "\n",
    "## Credit\n",
    "\n",
    "Parts of this work borrow from Rob Salgado's [excellent NMF tutorial](https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45), as well as Ravish Chawla's [NMF tutorial](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/stacy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"../Data/RedditNews.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News\n",
       "0  A 117-year-old woman in Mexico City finally re...\n",
       "1   IMF chief backs Athens as permanent Olympic host\n",
       "2  The president of France says if Brexit won, so...\n",
       "3  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting just the headlines for our corpus\n",
    "headlines = data[['News']]\n",
    "del data\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lemmitize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v') # pos='v' means it peforms stemming with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and words shorter than 3 characters, then lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['The', 'president', 'of', 'France', 'says', 'if', 'Brexit', 'won,', 'so', 'can', 'Donald', 'Trump']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['president', 'france', 'say', 'brexit', 'donald', 'trump']\n"
     ]
    }
   ],
   "source": [
    "sample = headlines['News'][2]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [year, woman, mexico, city, finally, receive, ...\n",
       "1      [chief, back, athens, permanent, olympic, host]\n",
       "2      [president, france, say, brexit, donald, trump]\n",
       "3    [british, police, hours, notice, threaten, hun...\n",
       "4    [nobel, laureates, urge, greenpeace, stop, opp...\n",
       "Name: cleaned_headlines, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines['cleaned_headlines']  = headlines['News'].map(preprocess)\n",
    "headlines['cleaned_headlines'][:5] # Check the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and transform the text using TF-IDF\n",
    "Sentence here about why we use TF-IDF in this case and not BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the headlines using TF-IDF to create features to train our model on\n",
    "texts = headlines['cleaned_headlines']\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "\n",
    "'''\n",
    "Filter out irrelevant words:\n",
    "Keep tokens that appear in at least 15 documents\n",
    "Keep only the 100,000 most frequent tokens\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, keep_n=100000)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3, # ignore words that appear in less than 3 articles\n",
    "    max_df=0.85, # ignore words that appear in more than 85% of articles\n",
    "    max_features=5000, # limit the number of important words to 5,000\n",
    "    ngram_range=(1, 2), # look for both words and two-word phrases\n",
    "    preprocessor=' '.join # join the tokenized words instead of creating a list\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a step here creating train-test splits (or should it come before we do TF-IDF?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit the NMF model on our headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=10, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create NMF model and fit it\n",
    "# We are using ten topic groupings here so it is directly comparable to the LDA model's performance\n",
    "\n",
    "'''\n",
    "Nonnegative Double Singular Value Decomposition (NNDSVD) [Boutsidis2007] \n",
    "is a new method designed to enhance the initialization stage of the nonnegative matrix factorization.\n",
    "\n",
    "NNDSVD is well suited to initialize NMF algorithms with sparse factors. - http://nimfa.biolab.si/nimfa.methods.seeding.nndsvd.html\n",
    "'''\n",
    "model = NMF(n_components=10, init='nndsvd')\n",
    "model.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 1</th>\n",
       "      <th>Topic # 2</th>\n",
       "      <th>Topic # 3</th>\n",
       "      <th>Topic # 4</th>\n",
       "      <th>Topic # 5</th>\n",
       "      <th>Topic # 6</th>\n",
       "      <th>Topic # 7</th>\n",
       "      <th>Topic # 8</th>\n",
       "      <th>Topic # 9</th>\n",
       "      <th>Topic #10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police</td>\n",
       "      <td>korea</td>\n",
       "      <td>israel</td>\n",
       "      <td>kill</td>\n",
       "      <td>russia</td>\n",
       "      <td>china</td>\n",
       "      <td>iran</td>\n",
       "      <td>world</td>\n",
       "      <td>say</td>\n",
       "      <td>saudi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>protest</td>\n",
       "      <td>north</td>\n",
       "      <td>gaza</td>\n",
       "      <td>attack</td>\n",
       "      <td>ukraine</td>\n",
       "      <td>chinese</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>bank</td>\n",
       "      <td>minister</td>\n",
       "      <td>arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>government</td>\n",
       "      <td>north korea</td>\n",
       "      <td>israeli</td>\n",
       "      <td>pakistan</td>\n",
       "      <td>russian</td>\n",
       "      <td>japan</td>\n",
       "      <td>attack</td>\n",
       "      <td>news</td>\n",
       "      <td>right</td>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>year</td>\n",
       "      <td>south</td>\n",
       "      <td>palestinian</td>\n",
       "      <td>bomb</td>\n",
       "      <td>putin</td>\n",
       "      <td>india</td>\n",
       "      <td>iranian</td>\n",
       "      <td>largest</td>\n",
       "      <td>prime</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arrest</td>\n",
       "      <td>south korea</td>\n",
       "      <td>hamas</td>\n",
       "      <td>people</td>\n",
       "      <td>syria</td>\n",
       "      <td>south china</td>\n",
       "      <td>iran nuclear</td>\n",
       "      <td>world bank</td>\n",
       "      <td>prime minister</td>\n",
       "      <td>yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>years</td>\n",
       "      <td>korean</td>\n",
       "      <td>palestinians</td>\n",
       "      <td>strike</td>\n",
       "      <td>military</td>\n",
       "      <td>build</td>\n",
       "      <td>sanction</td>\n",
       "      <td>global</td>\n",
       "      <td>human</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>people</td>\n",
       "      <td>north korean</td>\n",
       "      <td>bank</td>\n",
       "      <td>soldier</td>\n",
       "      <td>nato</td>\n",
       "      <td>beijing</td>\n",
       "      <td>weapons</td>\n",
       "      <td>countries</td>\n",
       "      <td>human right</td>\n",
       "      <td>behead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>state</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>west bank</td>\n",
       "      <td>isis</td>\n",
       "      <td>georgia</td>\n",
       "      <td>south</td>\n",
       "      <td>plant</td>\n",
       "      <td>world news</td>\n",
       "      <td>president</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>drug</td>\n",
       "      <td>jong</td>\n",
       "      <td>west</td>\n",
       "      <td>syria</td>\n",
       "      <td>warn</td>\n",
       "      <td>china say</td>\n",
       "      <td>power</td>\n",
       "      <td>change</td>\n",
       "      <td>state</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>force</td>\n",
       "      <td>launch</td>\n",
       "      <td>rocket</td>\n",
       "      <td>civilians</td>\n",
       "      <td>troop</td>\n",
       "      <td>billion</td>\n",
       "      <td>deal</td>\n",
       "      <td>biggest</td>\n",
       "      <td>official</td>\n",
       "      <td>human right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic # 1     Topic # 2     Topic # 3  Topic # 4 Topic # 5    Topic # 6  \\\n",
       "0      police         korea        israel       kill    russia        china   \n",
       "1     protest         north          gaza     attack   ukraine      chinese   \n",
       "2  government   north korea       israeli   pakistan   russian        japan   \n",
       "3        year         south   palestinian       bomb     putin        india   \n",
       "4      arrest   south korea         hamas     people     syria  south china   \n",
       "5       years        korean  palestinians     strike  military        build   \n",
       "6      people  north korean          bank    soldier      nato      beijing   \n",
       "7       state       nuclear     west bank       isis   georgia        south   \n",
       "8        drug          jong          west      syria      warn    china say   \n",
       "9       force        launch        rocket  civilians     troop      billion   \n",
       "\n",
       "      Topic # 7   Topic # 8       Topic # 9     Topic #10  \n",
       "0          iran       world             say         saudi  \n",
       "1       nuclear        bank        minister        arabia  \n",
       "2        attack        news           right  saudi arabia  \n",
       "3       iranian     largest           prime         women  \n",
       "4  iran nuclear  world bank  prime minister         yemen  \n",
       "5      sanction      global           human         right  \n",
       "6       weapons   countries     human right        behead  \n",
       "7         plant  world news       president          king  \n",
       "8         power      change           state         human  \n",
       "9          deal     biggest        official   human right  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the topics to visually inspect them\n",
    "\n",
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(10):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic #' + '{:2d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "get_nmf_topics(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, these topics seem to be a lot more coherent than what the LDA model produced. There's not only better in-topic coherence (i.e. the words relate to each other well) but also distinctions between topics (i.e. there's less keyword overlap from topic to topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new data with the fitted models\n",
    "# tfidf_new = tfidf_vectorizer.transform(new_texts)\n",
    "# X_new = nmf.transform(tfidf_new)\n",
    "\n",
    "# # Get the top predicted topic\n",
    "# predicted_topics = [np.argsort(each)[::-1][0] for each in X_new]\n",
    "\n",
    "# # Add to the df\n",
    "# df_new['pred_topic_num'] = predicted_topics\n",
    "\n",
    "# df_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
